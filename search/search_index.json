{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#about-me","title":"About Me","text":"<p>Experienced Cloud/Software Engineer with 17+ years experience.  Passionate about open-source technologies, Amazon Web Services, and small start-ups. Seeking to contribute to a tech-focused company or a non-profit organization solving interesting challenges. U.S. Army veteran.</p> <ul> <li>LinkedIn</li> <li>GitHub</li> <li>Credily</li> </ul> <p>If you have any questions, please reach out to me at will@samswebs.com</p>"},{"location":"#professional-activities","title":"Professional Activities","text":""},{"location":"#speaker","title":"Speaker","text":"<ul> <li>Iron Python w/ Visual Studio<ul> <li>PyCarolinas @ University of North Carolina, 2012</li> </ul> </li> </ul>"},{"location":"functional-versus-oop/","title":"Paradigm Discussion - Functional versus Object-Oriented Programming","text":"<p>First, let's talk about the elephant in the room: I'm inclined to think when it comes to building modern backends, especially highly concurrent, and distributed systems, you want to stick to functional programming concepts.  This was why I settled on TypeScript, Rust, and Go for the backends comparison discussion.  I'm not saying that you can't build a highly concurrent and distributed system with C#, Java, or Python, but I think you'll have an easier time with TypeScript, Rust, and Go.  While the latter languages mentioned aren't strict functional languages, these languages are well-suited to building backend systems that require high availability and fault tolerance.  </p> <p>In distributed, serverless applications, where the system may be composed of multiple processes or functions running on different machines, the potential for concurrency issues is even greater. This is because each process or function may be running independently and concurrently, with the potential for shared data or resources. In this context, functional programming can be particularly useful for reducing the complexity of concurrent and distributed systems, by emphasizing immutability and pure functions that avoid shared state and side effects.</p>"},{"location":"functional-versus-oop/#functional-programming","title":"Functional Programming","text":"<p>Functional programming emphasizes immutability, pure functions, and higher-order functions, which can lead to code that is easier to reason about, test, and parallelize. Functional programming is also well-suited for applications that deal with large amounts of data, such as data analysis, machine learning, and scientific computing. However, functional programming can have a steeper learning curve, and some developers may find it more difficult to write and maintain functional code compared to object-oriented code.  There are 3 key tenets of functional programming:</p> <ul> <li>Immutability: Immutable data cannot be changed after it is created. This makes it easier to reason about the state of a program, and it can also lead to better performance because the compiler can optimize away unnecessary copies of data.</li> <li>Pure functions: A pure function is a function that has no side effects and always returns the same output given the same input. Pure functions are easier to reason about and test, and they can be easily parallelized.</li> <li>Higher-order functions: A higher-order function is a function that takes a function as an argument or returns a function as a result. Higher-order functions can be used to abstract common patterns, which can lead to more modular and reusable code.</li> </ul> <p>It's easy to see how these tenets can lead to code that is easier to reason about, test, and parallelize.  For example, consider the following code:</p> <pre><code>const numbers = [1, 2, 3, 4, 5];\nconst doubled = numbers.map((number) =&gt; number * 2);\nconsole.log(doubled);\n</code></pre> <p>This code uses the <code>map</code> function to double each number in the array. The <code>map</code> function takes a function as an argument, and it returns a new array containing the results of calling the function on each element in the original array. In this case, the function passed to <code>map</code> is a pure function that doubles its argument. This code is easy to reason about because it's clear that the <code>doubled</code> array contains the same number of elements as the <code>numbers</code> array, and it's also clear that the elements in the <code>doubled</code> array are twice as large as the elements in the <code>numbers</code> array.</p>"},{"location":"functional-versus-oop/#object-oriented-programming-oop","title":"Object-Oriented Programming (OOP)","text":"<p>OOP concepts can cause problems in highly concurrent and distributed systems because of its general reliance on mutable state and shared data. In a concurrent system, multiple threads or processes may need to access and modify the same data simultaneously, which can lead to race conditions, deadlocks, and other types of synchronization issues.  This is not to say that OOP is inherently bad or unsuitable, but focusing on immutability and pure functions will mitigate the aforementioned issues.</p> <p>While OOP has its perks, like modularity, reusability, and extensibility, it can also lead to tightly coupled code, which makes it harder to understand how the entire system behaves compared to functional programming.  The 3 key tenets of object-oriented programming are:</p> <ul> <li>Encapsulation: Encapsulation is the process of hiding the implementation details of a class from the rest of the program. This makes it easier to change the implementation of a class without breaking other parts of the program that depend on that class.</li> <li>Inheritance: Inheritance is the process of creating a new class from an existing class. The new class inherits the behavior of the existing class, and it can also add new behavior. Inheritance can be used to reduce duplication in a program by sharing behavior across multiple classes.</li> <li>Polymorphism: Polymorphism is the process of reusing the same interface for multiple types. This can lead to more modular and reusable code because a function can operate on different types of data as long as they all implement the same interface.</li> </ul>"},{"location":"functional-versus-oop/#in-defense-of-c-java-and-python","title":"In Defense of C#, Java, and Python","text":"<p>Although languages such as C#, Java, and Python were not considered for the aforementioned backends comparison discussion, they are widely used for building backend systems.  Also, they do offer some support for functional programming concepts.</p> <p>A particular language I want to avoid completely shitting on is Python.  It has several libraries and tools that enable concurrent and asynchronous programming. Additionally, it offers multiprocessing support, which can be useful for distributed applications. While Python is not as low-level as Rust or as explicitly designed for concurrency as Go, it's still a viable choice for many distributed systems, particularly when ease of development and code readability are essential.  Python is interpreted, so it's very slow.  However, with the use of ctypes, you can call C code from Python, which can help mitigate the performance issues.  For example, a good use-case would be AWS IoT Greengrass, where you can write your business logic in Python, and then call C code for the performance critical parts.</p> <p>In addition (more self rebuttal), usage of a C# framework like ASP .NET Core or a Java framework like OfficeFloor are more than capable in the right hands.</p>"},{"location":"functional-versus-oop/#other-languages-not-considered-ruby-and-elixir","title":"Other languages not considered - Ruby and Elixir","text":"<p>While Ruby and Elixir are both powerful languages that are well-suited for building highly concurrent and distributed systems, they do have some potential drawbacks that keeps them from moving the needle for me.</p> <p>One potential disadvantage of Ruby is its runtime performance. Ruby is an interpreted language, which can make it slower than compiled languages like C++ or Rust. Additionally, Ruby's garbage collector can sometimes cause performance issues, especially in large-scale systems. While Ruby has a large ecosystem of libraries and frameworks, it may not have the same level of support for building highly concurrent and distributed systems as other languages like Elixir.  A very timely article titled \"Whatever Happened to Ruby?\" was published when I started writing this document, and it provides a good overview of the current state of Ruby.</p> <p>Elixir, on the other hand, is a relatively new language, and it may not have the same level of community support as more established languages. Additionally, while Elixir provides excellent support for concurrency, it may not be as well-suited for building systems that require a lot of mutable state. Finally, while Elixir runs on the Erlang virtual machine, which provides excellent support for fault tolerance and distributed computing, it may not have the same level of support for low-level systems programming as other languages like Rust.  Elixir has a different syntax and programming paradigm than many other popular languages, which can make it challenging for developers who are not familiar with functional programming to learn.  Still, if you want to learn more about the pros and cons of Elixir, I recommend reading this article titled \"The Pros &amp; Cons of Elixir Programming Language\".</p>"},{"location":"improving-the-dev-exp-further/","title":"Further Improving the Developer Experience with Argo CD and GitHub Actions","text":"<p>Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. With Argo CD, you can define your application deployment as a code, store it in a Git repository and use Argo CD to keep the deployed application in sync with the state specified in the Git repository.  Your GitHub repository is the source of truth for your application deployment. Argo CD continuously monitors your Git repository and automatically deploys your application whenever a change is detected.  The flow looks like this:</p> <p>---&gt; Merge PR Request ---&gt; Image Tag/Push to Registry Developer -------------------------&gt; Argo Sync ---&gt; Argo CD ---&gt; Kubernetes Cluster</p> <p>If that wasn't clear:  the developer initiates both actions concurrently.</p>"},{"location":"improving-the-dev-exp-further/#prerequisites","title":"Prerequisites","text":"<p>Improving the Development Experience with Loft &amp; DevSpace - Completion of this previous tutorial is the base for this Argo CD tutorial.  The previous tutorial gets us acclimated with: Amazon Elastic Kubernetes Service, the AWS CLI, the <code>eksctl</code> tool, the <code>kubectl</code> command, Helm, DevSpace, and Docker.</p> <p>First things first, let's create an <code>ecr_deployer</code> user we'll need for some GitHub repositories we'll create later.  We'll do this now because we'll need to use the <code>eks_admin</code> account for the rest of this tutorial afterwards.</p> <pre><code>AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\naws iam create-user --user-name ecr-deployer\n\necho '{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"ecr:*\"\n        ],\n        \"Resource\": \"*\"\n    }\n]\n}' &gt;| ecr-policy.json\n\naws iam create-policy \\\n    --policy-name ecr-policy \\\n    --policy-document file://ecr-policy.json\n\naws iam attach-user-policy --user-name ecr-deployer --policy-arn arn:aws:iam::$AWS_ACCOUNT_ID:policy/ecr-policy\n\n# THE following will create credentials for the ecr-deployer user we created in the previous step.\n# We'll store these credentials in a file called `ecr-creds` for reference later.\necho \"DEPLOYER_CREDS=$(aws iam create-access-key --user-name ecr-deployer --query 'AccessKey.{AccessKeyId:AccessKeyId,SecretAccessKey:SecretAccessKey}' --output text)\" &gt;| ecr-creds.sh\n</code></pre> <p>Building off the previous tutorial, we'll assume the <code>eks-cluster-role</code> for the <code>eks-admin</code> credentials. Both were created in the previous tutorial:</p> <pre><code>AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\nTEMP_CREDS=$(aws sts assume-role \\\n    --role-arn arn:aws:iam::$AWS_ACCOUNT_ID:role/eks-cluster-role \\\n    --duration-seconds 3600 \\\n    --role-session-name cluster-60-minute-session \\\n    --profile eks-admin)\n\n# REMINDER: these credentials will persist in your shell environment for the duration of the session\n# You can unset them by running the following commands: unset AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN.\n# Otherwise, your default and other profiles will not work until you restart your shell.\nexport AWS_ACCESS_KEY_ID=$(echo $TEMP_CREDS | jq -r '.Credentials.AccessKeyId')\nexport AWS_SECRET_ACCESS_KEY=$(echo $TEMP_CREDS | jq -r '.Credentials.SecretAccessKey')\nexport AWS_SESSION_TOKEN=$(echo $TEMP_CREDS | jq -r '.Credentials.SessionToken')\n</code></pre>"},{"location":"improving-the-dev-exp-further/#install-argo-cd","title":"Install Argo CD","text":"<p>We can install Argo CD using the Helm chart provided by the Argo CD team.  Unlike how we worked with Loft in the last tutorial, we will expose to the Internet using a LoadBalancer service.  This is because we will need to access Argo CD from our GitHub Actions workflow.</p> <pre><code>helm repo add argo https://argoproj.github.io/argo-helm\nhelm repo update\n\naws eks update-kubeconfig --name development\n\n# Using our root accout's profile we'll need to set the eks-admin credentials to create an IAM identity mapping for the eks-cluster-role\neksctl create iamidentitymapping \\\n    --cluster development \\\n    --arn arn:aws:iam::$AWS_ACCOUNT_ID:role/eks-cluster-role \\\n    --username eks-admin \\\n    --group system:masters \\\n    --region us-east-2 \\\n    --profile default\n\nkubectl create namespace argo-cd          # create namespace in cluster\nhelm install argo-cd argo/argo-cd --namespace argo-cd --set server.service.type=LoadBalancer\nARGO_SERVER=$(kubectl get service argo-cd-argocd-server -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' --namespace argo-cd)\n</code></pre> <p>Now that we've installed Argo CD in the argo-cd namespace, we can access the Argo CD UI at whatever is stored in the <code>ARGOCD_SERVER</code> environment variable.  The default username and password is <code>admin</code>, but to get the default password, you'll need to execute the following:</p> <pre><code>ARGO_PASSWORD=$(kubectl get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" --namespace argo-cd | base64 -d)\n</code></pre> <p>For this tutorial, I suggest you change the default password at <code>$ARGOCD_SERVER/user-info?changePassword=true</code>.  Afterwards, set the <code>ARGO_PASSWORD</code> environment variable to the new password.</p> <pre><code>export ARGO_PASSWORD=\"&lt;your new password&gt;\"\"\n</code></pre>"},{"location":"improving-the-dev-exp-further/#helm-charts-repository","title":"Helm Charts Repository","text":"<p>As mentioned in the \"Improving the Development Experience with Loft &amp; DevSpace\" article, I advise just creating a centralized repository for our Helm charts. For this tutorial, we'll use a Git repository to store our Helm charts.  We'll also use GitHub Actions to automatically build and push our Docker images to Amazon Container Registry, and we'll use Argo CD to deploy our application to our Kubernetes cluster.  The Kubernetes cluster can be a local cluster, such as Kind, or a remote cluster, such as Amazon Kubernetes Services.  For this tutorial, we'll use Kind.</p>"},{"location":"improving-the-dev-exp-further/#create-repositories-for-the-tutorial","title":"Create Repositories for the Tutorial","text":"<p>Let's create the repository for our Helm charts.  Let's re-brand as <code>Acme Technologies</code> for this example.</p> <pre><code>mkdir acme-organization &amp;&amp; cd $_\n</code></pre> <p>We will create a repository for our Helm charts.  Helm charts:</p> <pre><code>\nmkdir helm-deployments &amp;&amp; cd $_\necho \"# Helm Deployments\" &gt;| README.md\n\nwget -O .gitignore https://raw.githubusercontent.com/github/gitignore/main/Node.gitignore\necho '.env' &gt;| .dockerignore\n\ngit init . &amp;&amp; git add .\ngit commit -m \"Initial commit\"\n</code></pre> <p>For our upcoming 2 example applications, we'll eventually create Helm charts for them in the <code>helm-deployments</code>repository later.</p> <p>To avoid repeating ourselves, let's create both examples, their Docker images, and ECR repositories in one script.  This will work on Linux, Mac, and WSL.  If you're doing this in Powershell, this should be self-explanatory enough for you to follow along.</p> <pre><code># While still inside the root \"acme-organization\" folder,\n# let's create the repositories for 2 example applications.\n# We won't create the apps in this root, but we need to start this script there.\n# At the end of this script, you will see 3 folders inside the ace-organization folder:\n# 1. helm-deployments, 2. example-1, and 3. example-2\n# NOTE:  The 2 example repos can be public, but the helm-deployments repo will need to be private.\n\nAWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\nREPOSITORY_PATH=$AWS_ACCOUNT_ID.dkr.ecr.us-east-2.amazonaws.com\n\naws ecr get-login-password | docker login --username AWS --password-stdin $REPOSITORY_PATH\n\nfor i in {1..2}\ndo\n    cd ..\n    mkdir example-$i &amp;&amp; cd $_\n    mkdir -p .github/workflows\n\n    echo \"name: Build and Push to ECR\non:\n  push:\n    branches:\n      - main\njobs:\n  login-build-and-push-to-ecr:\n    runs-on: ubuntu-20.04\n    steps:\n      - id: configure-credentials\n        uses: aws-actions/configure-aws-credentials@v1\n        with:\n          aws-access-key-id: \\${{ secrets.AWS_ACCESS_KEY_ID }}\n          aws-secret-access-key: \\${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: us-east-2\n      - id: login-to-ecr\n        uses: aws-actions/amazon-ecr-login@v1\n      - id: checkout\n        uses: actions/checkout@v2\n      - id: build-and-push-to-ecr\n        run: |\n          docker build -t example-1:latest .\n          docker tag example-$i:latest \\${{ secrets.REPOSITORY_PATH }}/example-$i:latest\n          docker push \\${{ secrets.REPOSITORY_PATH }}/example-$i:latest\n        env:\n          ECR_REGISTRY: \\${{ steps.login-to-ecr.outputs.registry }}\n      - id: download-argo-cd-cli\n        uses: clowdhaus/argo-cd-action/@main\n        env:\n          # Only required for first step in job where API is called\n          # All subsequent setps in a job will not re-download the CLI\n          GITHUB_TOKEN: \\${{ secrets.GITHUB_TOKEN }}\n        with:\n          command: version\n          options: --client\n      - id: login-to-argo-cd\n        uses: clowdhaus/argo-cd-action/@main\n        with:\n          command: login\n          options: --username \\${{ secrets.ARGO_USERNAME }} --password \\${{ secrets.ARGO_PASSWORD }} --insecure \\${{ secrets.ARGO_SERVER }}\n      - id: trigger-restart-of-deployment\n        uses: clowdhaus/argo-cd-action/@main\n        with:\n          command: app actions run example-$i restart\n          options: --kind Deployment\n      - id: trigger-sync\n        uses: clowdhaus/argo-cd-action/@main\n        with:\n          command: app sync example-$i\n          options: --force\" &gt;| .github/workflows/build_and_push.yml\n\n        echo \"const http = require('http');\nconst port = 300$i;\n\nconst server = http.createServer((req, res) =&gt; {\n    res.statusCode = 200;\n    res.setHeader('Content-Type', 'text/plain');\n    res.end('Example-$i Application - Hello, world!\\n');\n});\n\nserver.listen(port, hostname, () =&gt; {\nconsole.log(\\`Server running at http://localhost:\\${port}/\\`);\n});\" &gt;| app.js\n\n        echo \"FROM node:18-alpine\nWORKDIR /app\nCOPY . .\nEXPOSE 300$i\nCMD [ \"node\", \"app.js\" ]\" &gt;| Dockerfile\n\n    echo \"# Example $i\" &gt;&gt; README.md\n    git init . &amp;&amp; git add .\n    git commit -m \"Initial commit\"\n\n    aws ecr create-repository --repository-name example-$i\n\n    docker build -t example-$i:latest .\n    docker tag example-$i:latest $REPOSITORY_PATH/example-$i:latest\n    docker push $REPOSITORY_PATH/example-$i:latest\ndone\n</code></pre> <p>Afterward, push both examples and the Helm charts repository to GitHub. These steps will need to occur on GitHub.</p>"},{"location":"improving-the-dev-exp-further/#create-github-actions-secrets","title":"Create GitHub Actions Secrets","text":"<p>Now we'll create the secrets for our GitHub Actions. We can do this programaticaly using a GitHub PAT (personal access token) and GitHub's CLI, but we'll do this in the GitHub UI  for brevity. Create the following secrets using the values displayed from the following commands:</p> <pre><code>echo ARGO_PASSWORD=$ARGO_PASSWORD\necho ARGO_SERVER=$ARGO_SERVER\necho ARGO_USERNAME=admin      # This is the default username for Argo CD but of course we can change it.\n\n# THE following will create credentials for the ecr-deployer user we created in the previous step.\nDEPLOYER_CREDS=$(aws iam create-access-key --user-name ecr-deployer --query 'AccessKey.{AccessKeyId:AccessKeyId,SecretAccessKey:SecretAccessKey}' --output text)\necho AWS_ACCESS_KEY_ID=$(echo $DEPLOYER_CREDS| awk '{print $1}')\necho AWS_SECRET_ACCESS_KEY=$(echo $DEPLOYER_CREDS| awk '{print $2}')\n\nAWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\nREPOSITORY_PATH=$AWS_ACCOUNT_ID.dkr.ecr.us-east-2.amazonaws.com\n</code></pre> <p>We'll only need to create these secrets for the example-1 and example-2 repositories.</p>"},{"location":"improving-the-dev-exp-further/#getting-started-with-argocd","title":"Getting Started with ArgoCD","text":"<p>The Argo CD CLI will need to be installed at this point.  To do so, executed <code>argocd login --username admin --password $ARGO_PASSWORD --insecure $ARGO_SERVER</code>.</p> <p>The end-goal is to deploy to our Kubernetes cluster when new images are pushed to AWS ECR.  We'll need to deploy to Kubernetes first and then subsequently create apps in Argo CD.  To baseline both, we can achieve this by utilizing Helm charts. Change your current directory to the <code>helm-deployments</code> repository and execute the following commands:</p> <pre><code>for i in {1..2}\ndo \n    helm create example-$i\n    sed -i \"s/repository: nginx/repository: $REPOSITORY_PATH\\/example-$i/g\" example-$i/values.yaml\n    sed -i \"s/tag: \\\"\\\"/tag: \\\"latest\\\"/g\" example-$i/values.yaml\n    sed -i \"s/port: 80/port: 300$i/g\" example-$i/values.yaml\n    sed -i \"nameOverride: \\\"\\\"\\/nameOverride: \\\"example-$i-app\\\"/g\" example-$i/values.yaml\n    sed -i \"fullnameOverride: \\\"\\\"\\/fullnameOverride: \\\"example-$i-chart\\\"/g\" example-$i/values.yaml\n    sed -i \"name: \\\"\\\"\\/name: \\\"example-$i\\\"/g\" example-$i/values.yaml\n\n    # Replace the deployment.yml file with the following:\n    # Note:  \"imagePullPolicy: Always\" is required for ArgoCD to pull the latest image from ECR\n    echo \"apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: example-$i\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: example-$i\n  template:\n    metadata:\n      labels:\n        app: example-$i\n    spec:\n      containers:\n      - name: example-$i\n        image: $REPOSITORY_PATH/example-$i:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 300$i\" &gt;| example-$i/templates/deployment.yaml\n\n    # Install the application via Helm chart to Kubernetes cluster\n    # This must be done before creeating app in Argo CD\n    helm install example-$i-chart example-$i/ --values example-$i/values.yaml --namespace loft-default-v-tutorial-vcluster\ndone\n</code></pre>"},{"location":"improving-the-dev-exp-further/#create-applications-in-argocd","title":"Create Applications in ArgoCD","text":"<p>Let's create the applications in ArgoCD using the ArgoCD CLI.  I found many tutorials using the <code>argocd-image-updater</code> tool but I found just using the ArgoCD GitHub action to be sufficient.  As seen in the workflows we created earlier, all we need was to push to ECR and then trigger re-deployment/sync.</p> <p>Now, let's create the applications in ArgoCD:</p> <pre><code>YOUR_GITHUB_SSH_KEY=~/.ssh/id_ed25519\nargocd repo add git@github.com:WillSams/helm-deployments.git --ssh-private-key-path $YOUR_GITHUB_SSH_KEY\nCLUSTER_ENDPOINT=$(aws eks describe-cluster --name development --query cluster.endpoint --output text)\n# create the applications in ArgoCD\nfor i in {1..2}\ndo \n    argocd app create example-$i \\\n    --repo git@github.com:WillSams/helm-deployments.git \\\n    --path example-$i \\\n    --dest-namespace loft-default-v-tutorial-vcluster   \\\n    --dest-server $CLUSTER_ENDPOINT \\\n    --sync-policy automated \\\n    --auto-prune \\\n    --server $ARGO_SERVER\ndone\n</code></pre> <p>To verify that the applications were created, we can look at the ArgoCD UI.  We should see the beautiful applications we created. On this dashboard, we can view the health of the applications and the sync status.  Let's port-forward example-1 and example-2 to our local machine:</p> <pre><code>kubectl port-forward deployment/example-1 3001:3001 -n loft-default-v-tutorial-vcluster &amp;\nkubectl port-forward deployment/example-2 3002:3002 -n loft-default-v-tutorial-vcluster &amp;\n</code></pre> <p>We can run <code>curl localhost:3001</code> to verify output.     <code>`` Now, let's make a change to both examples and trigger a sync.  We can do this by changing the</code>app.js<code>file and pushing the change to GitHub.  We should see the sync status change to</code>Syncing<code>and then</code>Synced<code>in the ArgoCD UI.  We can also view the logs of the sync by clicking on the</code>example-1<code>application and then clicking on the</code>Logs` tab.</p> <pre><code>for i in {1..2}\ndo\n    sed -i \"s/Example-$i Application - Hello, world/Example-$i Application - Modified and synced/g\" example-$i/app.js\n    git add .\n    git commit -m \"Update app.js\"\n    git push\ndone\n</code></pre> <p>We can also see the logs in Lofts UI or by running <code>kubectl logs -n argo-cd -l app.kubernetes.io/name=argocd-server -f</code>.  If you re-run <code>curl localhost:3001</code> you should see the change we made after you re-forward the port.  Since we did a re-deploy before the sync, the port-forward will need to be re-established.</p>"},{"location":"improving-the-dev-exp-further/#conclusion","title":"Conclusion","text":"<p>In this tutorial, we learned how to use ArgoCD to deploy applications to the Kubernetes cluster, how to use GitHub actions to trigger a sync in ArgoCD when new images are pushed to AWS ECR, and how to use the ArgoCD CLI to create applications in ArgoCD.  In combination with Loft and DevSpace, we can now create a full CI/CD pipeline for our applications while abstracting away the complexities of Kubernetes.  In the previous tutorial I stated that the typical workflow for a developer would be <code>devspace dev</code> or <code>devspace deploy</code> but with ArgoCD, we can definitely only say only <code>devspace dev</code> since we can now use ArgoCD to deploy our applications to Kubernetes.  This is a huge win for developers and DevOps our devops strategy alike.</p>"},{"location":"improving-the-dev-exp/","title":"Improving the Development Experience with Loft &amp; DevSpace","text":""},{"location":"improving-the-dev-exp/#introduction","title":"Introduction","text":"<p>The success of any software project relies on providing developers with the tools and resources they need to work efficiently and effectively. This is particularly true when it comes to Kubernetes, which is a powerful platform for building and deploying applications. However, Kubernetes can be complex and challenging to work with, especially for developers who are new to the platform. </p> <p>As a result, providing a good developer experience is essential to ensure successful Kubernetes projects.  In order to provide a good developer experience with Kubernetes, it's important to offer tools and resources that help developers work efficiently and effectively.  Let's discuss an excellent tool to help improve the developer experience with Kubernetes: Loft.</p>"},{"location":"improving-the-dev-exp/#what-is-loft","title":"What is Loft?","text":"<p>Loft is a platform that can help improve the development experience for Kubernetes, but it does so by simplifying cluster management. By providing a self-service portal for developers and teams to create and manage their own namespaces and resources, Loft streamlines the deployment process and reduces the complexity of managing Kubernetes clusters. This means that developers can focus more on coding and less on cluster management.  Loft usese tools such a DevSpace and vcluster to create a comprehensive development environment that allows developers to build, test, and deploy applications directly in Kubernetes, without having to worry about the underlying infrastructure.</p> <p>With Loft, developers can create and manage their own isolated environments, work collaboratively with team members, and deploy applications with confidence, knowing that they are using a secure, reliable, and scalable platform.</p>"},{"location":"improving-the-dev-exp/#devspace-and-vclusters-whats-are-those","title":"DevSpace and vclusters?  What's are those?","text":"<p>DevSpace is a developer tool that helps streamline the Kubernetes development workflow by providing a local development environment that's synced with a remote Kubernetes cluster. With features like hot reloading, debugging, and CI/CD integration, DevSpace allows developers to work on their code locally and see changes reflected in the cluster in real-time.  </p> <p>Another concept to not overlook is the fact Loft provides a virtual Kubernetes cluster environment called vCluster that runs inside a namespace of the underlying Kubernetes cluster. This allows users to test and deploy applications without the need for physical hardware or cloud resources, providing a lightweight and flexible development environment. Using virtual clusters can help reduce costs and improve productivity by providing a cost-effective alternative to creating separate full-blown clusters. Additionally, virtual clusters offer better multi-tenancy and isolation than regular namespaces, making them an ideal solution for teams that need to test and deploy applications in a shared environment.  We'll create one as part of this tutorial.</p>"},{"location":"improving-the-dev-exp/#pre-requisites","title":"Pre-requisites","text":"<p>Before we get started, we'll need to install the following tools:</p> <ul> <li>AWS CLI - configure it with your AWS credentials</li> <li>Docker</li> <li>kubectl</li> <li>eksctl</li> <li>Go - Optional; we'll be using the Docker containers for this tutorial</li> </ul>"},{"location":"improving-the-dev-exp/#enhancing-the-developer-experience","title":"Enhancing the developer experience","text":"<p>Loft is can be our all-in-one solution for managing your Kubernetes clusters and workloads. With its user-friendly interface, we can keep everything in check without breaking a sweat.  It can also be our one-stop-shop for handling namespaces, users, and RBAC policies. Loft also provides:</p> <ul> <li> <p>Collaboration features: Loft includes several collaboration features, such as team-based access controls, shared namespaces, and RBAC policies, which can help streamline collaboration among developers and teams.</p> </li> <li> <p>Plugin ecosystem: Loft has a growing plugin ecosystem, which provides additional functionality for managing your Kubernetes clusters and workloads, such as backup and restore tools, monitoring and logging integrations, and more.</p> </li> </ul> <p>Let's take a look at how we can use DevSpace and Loft together to improve the developer experience but first, let's create the AWS infrastructure we'll need for this tutorial.</p>"},{"location":"improving-the-dev-exp/#creating-the-aws-infrastructure","title":"Creating the AWS infrastructure","text":"<p>With an AWS account with console access, we'll need to create a few resources.  These resources won't qualify for the Amazon free tier so it's important we delete all resources at the end of the tutorial.  If we don't, leaving the Amazon Elastic Kubernetes Service or EKS cluster running continously will at least incur a $1 per day.  We'll be using the AWS CLI to create these resources, but we can use the AWS console if we prefer.  To install the AWS CLI, follow the instructions for your OS on the AWS CLI website. Once the AWS CLI is installed, you can verify that it is working by running the following <code>aws --version</code> command</p> <p>Next, we'll need to configure the AWS CLI with our AWS credentials. To do this, run the following command:</p> <pre><code>aws configure\n</code></pre> <p>This will prompt us for our AWS access key ID and secret access key, which we can find in the AWS console.  Once we've entered your credentials, we can verify that the AWS CLI is configured correctly by running the <code>aws sts get-caller-identity</code> command.  This should return our AWS account ID and user name.</p> <p>Now that we have the AWS CLI configured, we can create the resources we need for this tutorial.  First, create a VPC and one or more subnets for our EKS cluster. When creating our VPC and subnets, it's recommended to use separate subnets for our control plane and worker nodes, and to configure our VPC to use private IP addresses only.  To create a VPC and subnets, run the following commands:</p> <pre><code>VPC_ID=$(aws ec2 create-vpc \\\n  --cidr-block 10.0.0.0/16 \\\n  --tag-specification \"ResourceType=vpc,Tags=[{Key=Name,Value=MyVpc}]\" \\\n  --query 'Vpc.VpcId' \\\n  --output text)\n\nSUBNET_1=$(aws ec2 create-subnet \\\n  --vpc-id $VPC_ID \\\n  --cidr-block 10.0.1.0/24 \\\n  --availability-zone us-east-2a \\\n  --query 'Subnet.SubnetId' \\\n  --output text)\nSUBNET_2=$(aws ec2 create-subnet \\\n  --vpc-id $VPC_ID \\\n  --cidr-block 10.0.2.0/24 \\\n  --availability-zone us-east-2b \\\n  --query 'Subnet.SubnetId' \\\n  --output text)\n</code></pre> <p>Next, let's create an internet gateway and attach it to the VPC that you created previously. This will allow resources in the VPC to access the internet and vice versa:</p> <pre><code>IGW_ID=$(aws ec2 create-internet-gateway \\\n  --query 'InternetGateway.InternetGatewayId' \\\n  --output text)\n\naws ec2 attach-internet-gateway --internet-gateway-id $IGW_ID --vpc-id $VPC_ID\n</code></pre> <p>Create a route table for the VPC and associate it with the subnets we created earlier. This will allow resources in the subnets to communicate with each other and with the internet. You can create a route table and associate it with the subnets by running the following commands:</p> <pre><code>ROUTE_TABLE_ID=$(aws ec2 create-route-table \\\n  --vpc-id $VPC_ID \\\n  --query 'RouteTable.RouteTableId' \\\n  --output text)\n\naws ec2 associate-route-table --route-table-id $ROUTE_TABLE_ID --subnet-id $SUBNET_1\naws ec2 associate-route-table --route-table-id $ROUTE_TABLE_ID --subnet-id $SUBNET_2\naws ec2 create-route --route-table-id $ROUTE_TABLE_ID --destination-cidr-block 0.0.0.0/0 --gateway-id $IGW_ID\n</code></pre> <p>After creating our VPC and subnets, create a security group for our EKS cluster. When creating our security group, ensure that it allows inbound and outbound traffic on the necessary ports and protocols for our EKS cluster and any associated applications.  To create a security group, run the following command:</p> <pre><code>SG_ID=$(aws ec2 create-security-group \\\n  --group-name eks-cluster-sg \\\n  --description \"EKS cluster security group\" \\\n  --vpc-id $VPC_ID \\\n  --query 'GroupId' \\\n  --output text)\n\naws ec2 authorize-security-group-ingress \\\n  --group-id $SG_ID \\\n  --protocol tcp \\\n  --port 80 \\\n  --cidr 0.0.0.0/0\n</code></pre> <p>Next, let's create a cluster on EKS us the <code>eksctl</code> tool we installed as a pre-requisite. This will be the penulminate step we'll use for your AWS root or admin account for.  This step may take up to 20 minutes to complete:</p> <pre><code>eksctl create cluster \\\n  --name development \\\n  --nodegroup-name standard-workers \\\n  --node-type t3.medium \\\n  --nodes 3 \\\n  --nodes-min 1 \\\n  --nodes-max 4 \\\n  --managed\n</code></pre> <p>While we wait for that complete, let's open another terminal window. We will use this eks-cluster-role to create the EKS cluster to authenticate with the cluster.  Let's add a custom policy to this role:</p> <pre><code>aws iam create-user --user-name eks-admin\n\nAWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\necho '{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::'$AWS_ACCOUNT_ID':user/eks-admin\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n}' &gt;| eks-cluster-role-trust-policy.json\naws iam create-role --role-name eks-cluster-role --assume-role-policy-document file://eks-cluster-role-trust-policy.json\n\necho '{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"eks:DescribeCluster\",\n                \"ecr:BatchGetImage\",\n                \"ecr:BatchCheckLayerAvailability\",\n                \"ecr:CompleteLayerUpload\",\n                \"ecr:DescribeImages\",\n                \"ecr:DescribeRepositories\",\n                \"ecr:GetDownloadUrlForLayer\",\n                \"ecr:InitiateLayerUpload\",\n                \"ecr:ListImages\",\n                \"ecr:PutImage\",\n                \"ecr:UploadLayerPart\",\n                \"ecr:GetAuthorizationToken\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"eks:*\"\n                \"iam:ListRoles\",\n                \"sts:AssumeRole\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}' &gt;| eks-role-policy.json\naws iam put-role-policy --role-name eks-cluster-role --policy-name eks-role-policy --policy-document file://eks-role-policy.json\n\naws iam attach-role-policy --role-name eks-cluster-role --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy\naws iam attach-role-policy --role-name eks-cluster-role --policy-arn arn:aws:iam::aws:policy/AmazonEKSServicePolicy\naws iam attach-role-policy --role-name eks-cluster-role --policy-arn arn:aws:iam::aws:policy/AmazonEKSVPCResourceController\naws iam attach-role-policy --role-name eks-cluster-role --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy\naws iam attach-role-policy --role-name eks-cluster-role --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\n\necho '{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"sts:AssumeRole\",\n            \"Resource\": \"arn:aws:iam::'$AWS_ACCOUNT_ID':role/eks-cluster-role\"\n        }\n    ]\n}' &gt;| eks-admin-assume-cluster-role-policy.json\naws iam put-user-policy --user-name eks-admin --policy-name eks-admin --policy-document file://eks-admin-assume-cluster-role-policy.json\n</code></pre> <p>Now that we have the IAM role and policy created, let's create a user that will assume the eks-cluster-role.  To do this, run the following commands:</p> <pre><code># Create the AWS CLI profile for the EKS admin by exporting the credentials.\n# We'll then later use the '--profile eks-admin' flag to use these credentials in further steps\nCREDS=$(aws iam create-access-key --user-name eks-admin --query 'AccessKey.{AccessKeyId:AccessKeyId,SecretAccessKey:SecretAccessKey}' --output text)\nKEY_ID=$(echo $CREDS| awk '{print $1}')\nACCESS_KEY=$(echo $CREDS| awk '{print $2}')\n\necho \"\n[eks-admin]\naws_access_key_id=$KEY_ID\naws_secret_access_key=$ACCESS_KEY\nregion=us-east-2\" &gt;&gt; ~/.aws/credentials\n</code></pre> <p>We want to do work under the context of the IAM user and role we created earlier:</p> <pre><code># To use the temporary credentials to authenticate with the cluster, we'll assume the eks-cluster-role\n# using the eks-admin credentials. For the purposes of this tutorial, we'll use a 60 minute session\nTEMP_CREDS=$(aws sts assume-role \\\n  --role-arn arn:aws:iam::$AWS_ACCOUNT_ID:role/eks-cluster-role \\\n  --duration-seconds 3600 \\\n  --role-session-name eks-admin \\\n  --profile eks-admin)\n\n# NOTE: these credentials will persist in your shell environment for the duration of the session\n# You can unset them by running the following commands: unset AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN.\n# Otherwise, your default and other profiles will not work until you restart your shell.\nexport AWS_ACCESS_KEY_ID=$(echo $TEMP_CREDS | jq -r '.Credentials.AccessKeyId')\nexport AWS_SECRET_ACCESS_KEY=$(echo $TEMP_CREDS | jq -r '.Credentials.SecretAccessKey')\nexport AWS_SESSION_TOKEN=$(echo $TEMP_CREDS | jq -r '.Credentials.SessionToken')\n\n# All of your AWS CLI, kubectl, and Helm install/upgrade commands will now use the temporary credentials.\n# To use the eks-admin to authenticate with the cluster, we'll need to update the kubeconfig file while using the temporary credentials:\naws eks update-kubeconfig --name development\n\n# Using our root accout's profile we'll need to set the eks-admin credentials to create an IAM identity mapping for the eks-cluster-role\neksctl create iamidentitymapping \\\n  --cluster development \\\n  --arn arn:aws:iam::$AWS_ACCOUNT_ID:role/eks-cluster-role \\\n  --username eks-admin \\\n  --group system:masters \\\n  --region us-east-2 \\\n  --profile default\n</code></pre> <p>Next let's install Loft via Helm.</p>"},{"location":"improving-the-dev-exp/#installing-loft-via-helm","title":"Installing Loft via Helm","text":"<p>Helm is a package manager for Kubernetes that allows you to package and deploy Kubernetes applications. Helm charts are a collection of files that describe a related set of Kubernetes resources. Helm charts can be used to deploy applications, but they can also be used to deploy other types of Kubernetes resources which are out of scope for this article.</p> <p>To install Helm, follow the instructons for your OS on the Helm website. Once Helm is installed, you can verify that it is working by running the following command:</p> <pre><code>helm version\n</code></pre> <p>Helm charts are stored in a Helm chart repository, which is a collection of Helm charts. Helm chart repositories can be hosted on a public or private server, or they can be hosted on a cloud storage service, such as Amazon S3 or Google Cloud Storage. Helm chart repositories can be accessed using the Helm CLI, which can be installed using the following command:</p> <pre><code>curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash\n</code></pre> <p>Once the Helm CLI is installed, we can add a Helm chart repository.  To get started, we'll need to install Loft to our cluster for demonstration on how it would work on a remote instance.  To do this, we'll first need to add the Loft Helm repository to our local Helm installation.  To do this, run the following command in your terminal:</p> <pre><code>helm repo add loft https://charts.loft.sh\nhelm repo update\n</code></pre> <p>Using namespaces is useful when we need to logically separate workloads within a single cluster, but still want to share resources such as nodes and network infrastructure. Namespace-level resource quotas can also be applied to prevent any one workload from using up too much of the shared resources.</p> <p>While Kubernetes namespaces are used to isolate workloads and resources within a single cluster, creating separate clusters with creates isolated environments with their own Kubernetes API server, etcd storage, and worker nodes. Each cluster can have its own configuration, resource allocation, and security settings. Separate clusters are also useful when you need to create isolated environments for different purposes, such as development, testing, or production. Each cluster can have its own resources and configurations, making it easier to manage and isolate workloads.</p> <p>With that said, let's create a namespace for our loft resources with the <code>kubectl</code> command.  The <code>kubectl</code> command is a necessary command-line tool for interacting with Kubernetes clusters.  To create a these resources, run the following commands:</p> <pre><code>kubectl create namespace loft\nhelm install loft loft/loft --namespace loft\n</code></pre> <p>It may take the container a bit to start up, but once it's running, we can verify that it's running by running the following command:</p> <pre><code>kubectl get pods --namespace loft\n</code></pre>"},{"location":"improving-the-dev-exp/#getting-started-with-loft","title":"Getting Started with Loft","text":"<p>Once Loft is installed, let's expose it to our local machine so that we can access it.  We can then access the Loft UI by running the following command in our terminal:</p> <pre><code>kubectl port-forward service/loft 8080:80 --namespace loft &amp;\n</code></pre> <p>Now, we can access the Loft UI at https://localhost:8080. The default username and password is <code>admin</code> and the default password is <code>my-password</code>.</p> <p>After we've added user details, we can create a new cluster by clicking on the <code>Virtual Clusters</code> link in the sidebar.  vClusters in Loft provide a way to create separate virtual Kubernetes clusters within a single Kubernetes cluster.  Once we see the <code>Virtual Clusters Management</code> page, we can create a new virtual cluster by clicking on the <code>Create Virutal Cluster</code> button.  Next, select <code>Isolated Virtual Cluster Template</code> option.  In the subsequent dialog, if the YAML isn't shown by default, we can click on the <code>Show YAML</code> button at the bottom of the page, center-right.  Once the configuration appears, modify the <code>name</code> in the metadata block to <code>loft-default-v-tutorial-vcluster</code>.  </p> <p>Within the right-hand dialog, in the Add Permission to: field, select the user you created when you configured Loft on initial login.  Click the + button to add the user to the list.  We can now click the Create button to create the virtual cluster.</p>"},{"location":"improving-the-dev-exp/#getting-started-locally-with-devspace","title":"Getting started locally with DevSpace","text":"<p>Loft is particularly useful when working with DevSpace as it provides a streamlined development workflow that enables developers to work on their code locally and see changes reflected in the virtual cluster in real-time.</p> <p>By using DevSpace with Loft, developers can benefit from features such as hot reloading, debugging, and integration with popular IDEs like Visual Studio Code. These features can significantly enhance the developer experience and improve productivity, as developers can focus on writing code and testing their applications, rather than dealing with the complexities of Kubernetes infrastructure.</p> <p>However, it's worth noting that DevSpace can be used with any Kubernetes cluster, whether it's running locally (e.g., minikube, Kind, etc.) or in the cloud via EKS. DevSpace is a versatile tool that can be used to streamline the Kubernetes development workflow and provide a more efficient and productive development experience.</p> <p>Here are the general steps to get started:</p>"},{"location":"improving-the-dev-exp/#11","title":"1.1","text":"<p>Install DevSpace by following the installation instructions on the DevSpace website: https://devspace.sh/docs/getting-started/installation.  For example, on Linux, you can install DevSpace by running the following command in your terminal: <code>curl -L -o devspace \"https://github.com/loft-sh/devspace/releases/latest/download/devspace-linux-amd64\" &amp;&amp; sudo install -c -m 0755 devspace /usr/local/bin</code>.</p> <p>Once DevSpace is installed, you can verify that it's working by running the following command in your terminal:</p> <pre><code>devspace version\n</code></pre> <p>If verified, let's switch the namespace to the virtual cluster we created earlier.  To do this, run the following command in your terminal:</p> <pre><code>aws eks update-kubeconfig --name development  # update kubeconfig to point to your cluster\ndevspace use namespace loft-default-v-tutorial-vcluster \n</code></pre> <p>Yes, virtual clusters are namespeces.  They are a powerful feature of the Loft platform that allows you to create a dedicated namespace with its own set of resources and policies. This can help you isolate your application and manage your resources more efficiently. If you want to learn more about virtual clusters, you can check out the Loft documentation.</p>"},{"location":"improving-the-dev-exp/#12","title":"1.2","text":"<p>Once you have both DevSpace installed, let's create a new project.  For this example, let's use Go.  Follow these steps to create a Hello World project to deploy to our cluster:</p> <pre><code>mkdir hello-world &amp;&amp; cd $_\ngo mod init hello-world\necho 'package main\n\nimport (\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc handler(w http.ResponseWriter, r *http.Request) {\n    fmt.Fprintf(w, \"Hello World!\")\n}\n\nfunc main() {\n    fmt.Println(\"Started server on :3000\")\n\n    http.HandleFunc(\"/\", handler)\n    http.ListenAndServe(\":3000\", nil)\n}' &gt;| main.go\n\n# Now let's create a Makefile to help typing commands less painful\necho '.PHONY: build clean run deploy\n\nbuild:\n    go mod verify &amp;&amp; go mod tidy\n    go build -ldflags=\"-s -w\" -o bin/hello_world ./main.go\n\nclean:\n    rm -rf ./bin\n    mkdir -p ./bin\n\nrun: clean build\n    ./bin/hello_world' &gt;| Makefile\n\nmake run &amp; # run the server in the background\ncurl localhost:3000 # should return \"Hello World!\"\n\n# NOTE: Whenever you want stop the server running in the background, \n# execute `kill %1` in the terminal\n\n## Finally, let's build our Dockerfile\necho 'FROM golang:1.20-alpine as build\n\nWORKDIR /opt/app\n\n# cache dependencies\nCOPY go.*mod* ./\nRUN go mod download\n\n# build\nCOPY *.go ./\nRUN CGO_ENABLED=0 go build -ldflags=\"-s -w\" -o ./bin/hello_world ./main.go\n\n## Deploy\nFROM gcr.io/distroless/base-debian10\n\nWORKDIR /\nCOPY --from=build /opt/app/bin/hello_world /bin/hello_world\nEXPOSE 3000\nUSER nonroot:nonroot\nCMD [ \"./bin/hello_world\" ]' &gt;| Dockerfile\n\necho '*.sh\n.devspace/\nDockerfile' &gt;| .dockerignore\n\ngit init .\necho '\n*.swp' &gt;&gt; .gitignore\n\ngit add .\ngit commit -m \"Initial commit\"\n\n# build our Docker image\ndocker build -t go-hello-world:latest .\n\nAWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\nREPOSITORY_PATH=$AWS_ACCOUNT_ID.dkr.ecr.us-east-2.amazonaws.com\naws ecr create-repository --repository-name go-hello-world\naws ecr get-login-password | docker login --username AWS --password-stdin $REPOSITORY_PATH\n\ndocker tag go-hello-world:latest $REPOSITORY_PATH/go-hello-world:latest\ndocker push $REPOSITORY_PATH/go-hello-world:latest\n</code></pre> <p>To test our Docker image works as expected, execute <code>docker run -p 3000:3000 go-hello-world</code> in the terminal.  In another terminal window, <code>curl localhost:3000</code> to see the output.</p>"},{"location":"improving-the-dev-exp/#13","title":"1.3","text":"<p>Now that we have a Docker image that we can deploy to our cluster, let's create a Helm chart.</p> <pre><code>helm create go-hello-world\n\n# Replace the values.yml file with the following:\nsed -i \"s/repository: nginx/repository: $REPOSITORY_PATH\\/go-hello-world:latest/g\" go-hello-world/values.yaml\nsed -i 's/port: 80/port: 3000/g' go-hello-world/values.yaml\n\n# Replace the deployment.yml file with the following:\necho \"apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: go-hello-world\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: go-hello-world\n  template:\n    metadata:\n      labels:\n        app: go-hello-world\n    spec:\n      containers:\n      - name: go-hello-world\n        image: $REPOSITORY_PATH/go-hello-world:latest\n        ports:\n        - containerPort: 3000\" &gt;| go-hello-world/templates/deployment.yaml\n</code></pre>"},{"location":"improving-the-dev-exp/#14","title":"1.4","text":"<p>Let's create a new DevSpace project by running the following command in our terminal <code>devspace init</code>.  This will create a new DevSpace configuration file (e.g., <code>devspace.yaml</code>) in the current directory. When prompted, select the following options:</p> <pre><code>? Select a template:  *Go*\n? How do you want to deploy this project?  *Helm Chart*\n? Do you already have a Helm chart for this project?  *Yes*\n? Which Helm chart do you want to use?  *local*\n? Please enter the relative path to the Helm chart:  *./go-hello-world*\n? Do you want to develop this probject with DevSpace or just deploy it? *I want to develop this project and my current working dir contains the source code*\n? Which image do you want to develop with DevSpace?  Manually enter the image I want to work on\nManually enter the image I want to work on  *&lt;REPOSITORY-PATH&gt;go-hello-world:latest*\n? How should DevSpace build the container image for this project?  Use this existing Dockerfile: *./Dockerfile*\n</code></pre> <p>Note: replace <code>&lt;REPOSITORY-PATH&gt;</code> with the repository path from the previous step.</p>"},{"location":"improving-the-dev-exp/#15","title":"1.5","text":"<p>Finally, let's run <code>devspace dev</code> to start our local development environment.  This will deploy our application to the local kubernetes cluster in the virutal cluster namespace.  It will also sync our local files with the cluster, so any changes we make to our code will be reflected in the cluster in real-time.  If you get a '[screen is terminated]' message, run <code>devspace sync &amp;</code> to resolve the issue and then run <code>devspace dev</code> again.  You will then be logged into the container and can start developing your application with changes syncing to your local files in real-time.  </p> <p>In another terminal window, you can view the deployed app in the cluster by running the following:</p> <pre><code>kubectl get pods --namespace loft-default-v-tutorial-vcluster\n</code></pre> <p>For security reasons, the port of our application isn't forwarded.  To forward the port of our application, run the following:</p> <pre><code>kubectl port-forward deployment/go-hello-world 3000:3000 --namespace loft-default-v-tutorial-vcluster &amp;\ncurl localhost:3000  #  you should see \"Hello, world!\"\n</code></pre> <p>The purpose of running DevSpace is to create a local development environment that closely mirrors your other environments, allowing you to develop and test your application locally before deploying it to a higher shared environment (staging/qa/production). This is particularly useful in the context of Kubernetes, as it allows you to test your application in a containerized environment and ensure that it will work correctly when deployed to a cluster.  Changes you make to your local code should sync to your locally deployed container and vice-versa, allowing you to test your application in real-time.</p> <p>Other than using the terminal connection to your container, you can also use a web interface to make changes, view the logs of your application, and debug any issues.  To access the web interface, run <code>devspace ui</code> in the terminal.  This will open a new browser window with the DevSpace dashboard.  You can view the logs of your application by clicking on the 'Logs' tab and selecting the 'go-hello-world' container.  You can also view the logs of your application by running <code>devspace logs go-hello-world</code> in the terminal.  However, it is very limited in functionality, so it is recommended that you use Loft's web interface instead.</p> <p>In addition, Devspace provides a number of other features that can be useful in a local development environment, such as automatically restarting your application when you make changes to your code (see, 'Configure Auto-Reloading'), providing access to logs and debugging information, and allowing you to easily switch between different versions of your application.</p> <p>Overall, DevSpace provides a powerful tool for local development and testing of Kubernetes applications, allowing you to catch and fix issues early in the development process and reduce the risk of issues when deploying to a production environment.</p>"},{"location":"improving-the-dev-exp/#integrating-with-cicd-tools","title":"Integrating with CI/CD tools","text":"<p>DevSpace can be integrated with a number of popular CI/CD tools, such as GitHub Actions, GitLab CI/CD, and Jenkins.  This allows you to automate the deployment of your application to a Kubernetes cluster, which can be useful in a number of scenarios, such as deploying your application to a staging environment after a pull request is merged, or deploying your application to a production environment after a release is created.</p> <p>Tools like Flux and Argo CD are also commonly used for automating the deployment of Kubernetes applications.  However, these tools are typically used for deploying applications to a production environment, as they are designed to be used in a continuous delivery (CD) pipeline, where the application is deployed to a production environment after a release is created.  Outside of the scope of this tutorial but worth mentioning.</p>"},{"location":"improving-the-dev-exp/#logging","title":"Logging","text":"<p>Devspace provides you with access to the logs generated by your application and its underlying infrastructure, such as the container runtime, the Kubernetes cluster, and any other services that your application depends on. These logs are typically used for debugging and troubleshooting purposes, as they contain valuable information about the behavior of your application and the environment it's running in.</p> <p>Additionally, Devspace allows you to configure log forwarding to external services such as Elasticsearch, Fluentd, or Datadog, where you can analyze and visualize your logs in more detail. This can be useful for monitoring and alerting purposes, as well as for gaining deeper insights into the behavior of your application and the environment it's running in.  This can be done by:</p> <pre><code>devspace logs -f\n</code></pre> <p>Subsequently, you should see output like the following:</p> <pre><code>&gt; devspace logs -f\ninfo Using namespace 'loft-default-v-tutorial-vcluster'\ninfo Using kube context 'arn:aws:eks:us-east-2:xxxxxx:cluster/development'\ninfo Printing logs of pod:container go-hello-world-devspace-xxxxx-xxxxx:go-hello-world\n</code></pre> <p>Devspace provides a powerful set of tools for logging and debugging your application, making it easier to diagnose issues and improve the quality of your code. By providing access to logs and debugging information, Devspace helps you get the most out of your local development environment, and streamlines the development process.</p>"},{"location":"improving-the-dev-exp/#profiles","title":"Profiles","text":"<p>We often need to test different versions of the code to see how they behave and to compare their performance when developing applications. For example, we may need to test a new feature or fix a bug in an older version of our code. Switching between different versions of our application can be time-consuming and error-prone, especially if you need to manually manage multiple code branches, containers, and environments.</p> <p>Devspace makes it easy to switch between different versions of our application by providing a feature called \"profiles\". A profile is a set of configuration files that define the environment variables, container images, and other parameters that our application needs to run. By defining different profiles for each version of our code, we can easily switch between them without having to manually manage multiple containers or environments.  For an example, for a monorepo we can have a profile for each service in the repo.  For example, in our <code>.devspace/config.yaml</code> file we can have the following:</p> <pre><code>images:\n  backend:\n    image: my-backend-image\n    tag: latest\n  frontend:\n    image: my-frontend-image\n    tag: latest\n</code></pre> <p>Subsequently, we can switch between different versions of our application by simply executing <code>devspace use profile &lt;profile-name&gt;</code>, where <code>&lt;profile-name&gt;</code> is the name of the profile we want to use.</p> <p>As an another example, we might define a \"production\" profile that uses the latest stable version of our code, and a \"development\" profile that uses the latest version of our code from the development branch. When we switch between these profiles, Devspace will automatically create or update the required containers, volumes, and other resources for us.</p> <p>Devspace's profile feature provides a powerful tool for managing and switching between different versions of your application, making it easier to test and iterate on your code. By allowing you to easily switch between different profiles, Devspace streamlines the development process and helps you get the most out of our local development environment.</p>"},{"location":"improving-the-dev-exp/#cleanup","title":"Cleanup","text":"<p>To clean up the resources created by this tutorial, run the following commands:</p> <pre><code>killall -s 9 kubectl  # kill the port forwarding process\nkillall -s 9 devspace  # kill the sync process\n\n# Delete the DevSpace deployment and vCluster VM\ndevspace purge &amp;&amp; devspace reset vars\ndevspace cleanup images\ndevspace use namespace default  # or whatever namespace you want to switch to\nkubectl delete namespace loft-default-v-tutorial-vcluster\n\neksctl delete cluster --name development &amp; # this will take a bit to do, so run it in the background or another terminal\n\n# Deleting the AWS resources we created\naws iam delete-role-policy --role-name eks-cluster-role --policy-name eks-role-policy\naws iam delete-role --role-name eks-cluster-role\nKEY_FOR_USER=$(aws iam list-access-keys --user-name eks-admin | jq -r '.AccessKeyMetadata[0].AccessKeyId')\naws iam delete-access-key --access-key-id $KEY_FOR_USER --user-name eks-admin\naws iam delete-user --user-name eks-admin\n\naws ec2 delete-security-group --group-id $SG_ID\naws ec2 delete-subnet --subnet-id $SUBNET_1\naws ec2 delete-subnet --subnet-id $SUBNET_2\naws ec2 delete-route  \\\n  --route-table-id $ROUTE_TABLE_ID \\\n  --destination-cidr-block 0.0.0.0/0\naws ec2 delete-route-table --route-table-id $ROUTE_TABLE_ID \naws ec2 detach-internet-gateway \\\n  --internet-gateway-id $IGW_ID \\\n  --vpc-id $VPC_ID\naws ec2 delete-internet-gateway --internet-gateway-id $IGW_ID\naws ec2 delete-vpc --vpc-id $VPC_ID\n</code></pre>"},{"location":"improving-the-dev-exp/#conclusion","title":"Conclusion","text":"<p>Long tutorial, but we've covered a lot of ground.  The only bits of this reflecting a developer's daily work experience is collaborating in Loft and the <code>devspace dev</code>/<code>devspace deploy</code> commands.  The rest of the commands are for setting up the environment and cleaning up after ourselves.</p> <p>In this tutorial, we learned how to use Loft with DevSpace to develop and test Kubernetes applications locally. We also learned how to use DevSpace to automate the deployment of our application to a Kubernetes cluster, and how to use DevSpace's profile feature to switch between different versions of our application.  However, the latter isn't really how we would use DevSpace for staging and production environments.  The next tutorial will cover how to bring everything we've discussed in this tutorial together to create a full CI/CD pipeline for our application using GitHub Actions and Argo CD.</p>"},{"location":"the-case-for-go-backends/","title":"Comparing TypeScript, Rust, and Go for Backends","text":"<p>Creating high-performing, scalable, and budget-friendly backends is pretty darn important for today's software. With all the buzz around serverless setups and cloud computing, your choice of programming language can seriously shake things up in how your app runs and what it costs. So, in this guide, we're going to put TypeScript, Rust, and Go under the microscope, checking out how they fare in terms of performance, scalability, and just how much fun they are for developers to work with. The goal? To suss out the pros and cons of each language, so we can pick the best one for our specific backend-building needs. Let's find find that sweet spot between performance, scalability, and cost-efficiency, all while keeping the developer experience great.</p>"},{"location":"the-case-for-go-backends/#proscons-of-typescript-rust-and-go","title":"Pros/Cons of TypeScript, Rust, and Go","text":""},{"location":"the-case-for-go-backends/#typescripts-proscons","title":"TypeScript's Pros/Cons","text":"<p>TypeScript is a handy tool for web app development, especially if we're already a JavaScript pro. However, there are some performance quirks and limitations could make it less ideal for those hefty, complex backends.</p> <ul> <li> <p>Pros:  One of the great things about TypeScript is how it's fairly painless for developers who know their way around JavaScript. With a big and vibrant community, along with a ton of libraries and tools, it's a go-to pick for building web apps. TypeScript doesn't skimp on the modern goodies either, supporting features like async/await, decorators, and interfaces for tackling even the trickiest projects. Plus, it handles small to medium-sized apps like a champ, delivering good performance.</p> </li> <li> <p>Cons:  Now, the not-so-great part about TypeScript is that it can slow things down a bit due to its type-checking and compilation processes, especially when we're dealing with larger applications. It's a high-level language, so it's not the go-to choice for low-level system programming. And when it comes to mega high-traffic applications, TypeScript might not be our choice if we're after peak performance.</p> </li> </ul>"},{"location":"the-case-for-go-backends/#rusts-proscons","title":"Rust's Pros/Cons","text":"<p>Rust's a powerhouse for building speedy and rock-solid backends. It's got a learning curve, and you might need to dig a little deeper for resources compared to the bigger language tribes.</p> <ul> <li> <p>Pros:  One of the big wins with Rust is its need-for-speed attitude and efficiency, all while being a memory-saving champ. It's a stickler for strong typing and offers zero-cost abstractions. That's a fancy way of saying it's perfect for the tech wizards crafting low-level system apps that demand top-notch performance. Plus, Rust's about safety, making sure our software stays reliable and secure. It's no slouch in the multitasking game either, which makes it a top pick for apps that juggle lots of tasks at once.</p> </li> <li> <p>Cons: Rust\u2014it's not the easiest nut to crack, especially if you're not well-versed in system programming. Rust's gang of developers and the stash of libraries and tools isn't as vast as some other languages, so you might have to roll up your sleeves to find solutions. Rust also can be a bit verbose, which might slow down your coding productivity until you become more familiar with it.</p> </li> </ul>"},{"location":"the-case-for-go-backends/#gos-proscons","title":"Go's Pros/Cons","text":"<p>Go is seemingly perfect for building super-fast backends that can handle tons of stuff at once. However, it's not all sunshine and rainbows; its simplicity comes at a cost.</p> <ul> <li> <p>Pros:  Go sips on memory like a pro, making it a great choice for crafting high-performance apps. It excellent at handling multiple tasks at once, which is handy for those apps with a lot going on. Plus, Go keeps it simple with it's clean syntax and a handy standard library. Also, highly recommended for medium to large-scale apps, striking a sweet balance between power and user-friendliness.</p> </li> <li> <p>Cons:  Now, the fine print:  Go is missing features other languages may have.  If you're the tool-hungry type, you might need to look around a bit more.  Also, Go's garbabge collection can slow things down in certain situations, which might not be great for some apps. o .</p> </li> </ul>"},{"location":"the-case-for-go-backends/#performance","title":"Performance","text":"<p>For high-performance backends and serverless applications, Rust and Go are both excellent options, while TypeScript may be better suited for smaller applications or projects that require greater ease of use and familiarity with JavaScript.</p>"},{"location":"the-case-for-go-backends/#typescripts-performance","title":"TypeScript's Performance","text":"<p>Many dig TypeScript because it's easy to use if you are familiar with JavaScript.  But here's the deal when it comes to speed: TypeScript might not be the fastest horse in the race. It gets converted into JavaScript, and that extra translation work can slow things down a smidge due to the type-checking and compilation overhead. The slowdown is usually no big deal, especially for your average-sized apps.</p> <p>In the following code snippet, we are using TypeScript to calculate the 45th Fibonacci number. The Fibonacci sequence is a series of numbers in which each number is the sum of the two preceding numbers. The first two numbers in the sequence are 0 and 1, and each subsequent number is the sum of the two preceding numbers. The 45th Fibonacci number is 1,134,903,170. In this code snippet, we are using recursion to calculate the 45th Fibonacci number. Just a heads up, depending on your computer's horsepower, this snippet might take up to 15 seconds to finish its job. If you're using a slower computer, it might take even longer.</p> <pre><code>function fibonacci(n: number): number {\n  if (n &lt; 2) {\n    return n;\n  }\n  return fibonacci(n - 1) + fibonacci(n - 2);\n}\n\nconst result = fibonacci(45);\nconsole.log(result);\n</code></pre>"},{"location":"the-case-for-go-backends/#rusts-performance","title":"Rust's Performance","text":"<p>Rust is all about being lightning-fast and staying safe when it comes to system programming. The secret sauce behind its performance? It's got zero-cost abstractions and high-level control over system resources.  When you've got a need for speed, especially in serverless functions where you want things to be as performant as possible, Rust seems like a logical choice.</p> <p>As an example, using Rust, we can calculate the 45th Fibonacci number in as little as 10 seconds.</p> <pre><code>fn fibonacci(n: u32) -&gt; u32 {\n    if n &lt; 2 {\n        return n;\n    }\n    fibonacci(n - 1) + fibonacci(n - 2)\n}\n\nfn main() {\n    let result = fibonacci(45);\n    println!(\"{}\", result);\n}\n</code></pre>"},{"location":"the-case-for-go-backends/#gos-performance","title":"Go's Performance","text":"<p>Performance-wise, Go is a star player, thanks to its speedy compilation and prioritization on concurrency game. As a result, Go is starting to become a popular choice for building high-performance serverless applications, thanks to its low memory footprint and fast execution speed.</p> <p>Using Go, we can calculate the 45th Fibonacci number in approximately 5 seconds.</p> <pre><code>package main\n\nimport \"fmt\"\n\nfunc fibonacci(n int) int {\n    if n &lt; 2 {\n        return n\n    }\n    return fibonacci(n-1) + fibonacci(n-2)\n}\n\nfunc main() {\n    result := fibonacci(45)\n    fmt.Println(result)\n}\n</code></pre> <p>Wait, isn't Rust supposed to be faster than Go?  Rust's ownership model ensures memory safety without the need for garbage collection, which can sometimes lead to better performance in scenarios where memory management is critical. However, the Rust implementation of the Fibonacci function uses a naive recursive algorithm, which can lead to a large number of stack frames being allocated.  In contrast, Go's garbage collector is highly optimized and can perform concurrent garbage collection, which can help manage the memory usage of the Fibonacci function.  In this particular scenario, the garbage collector in Go help manage the memory usage of the Fibonacci function more efficiently than Rust's ownership model, leading to better performance.</p> <p>For a deeper analysis of their performance, there is an entertaining and informative video on YouTube that compares TypeScript, Rust, and Go I suggest you to view if you are interested in learning more.</p>"},{"location":"the-case-for-go-backends/#developer-experience","title":"Developer Experience","text":"<p>TypeScript, Rust, and Go are also known for providing a great developer experience, with features that can make it easier and more enjoyable for developers to write and maintain code. However, there are also potential challenges and drawbacks that developers may face when working with these languages.:</p> <ul> <li> <p>TypeScript has a strong focus on developer productivity, with features like optional chaining and nullish coalescing that can make code more concise and readable.  However:</p> <ul> <li>Type annotations can add complexity and verbosity to the code, especially for simple or small-scale projects.  For example: <code>const add = (a: number, b: number): number =&gt; a + b;</code> versus <code>const add = (a, b) =&gt; a + b;</code></li> <li>The learning curve for TypeScript can be steeper than for other languages, especially for developers who are not already familiar with JavaScript.</li> <li>Code editors and other development tools may require additional configuration to work properly with TypeScript, which can be time-consuming.</li> </ul> </li> <li> <p>Rust has a focus on performance and efficiency, with low-level control over system resources and efficient memory management. However:</p> <ul> <li>Rust's strong type system and borrow checker can be challenging for developers who are not used to working with memory-safe languages, especially when dealing with complex data structures or concurrency.</li> <li>The syntax and language features in Rust can be more complex and verbose than in other languages, which can make it harder for some developers to learn and write code.</li> <li>The Rust compiler can be slower than some other languages, especially when dealing with large codebases or complex projects.</li> </ul> </li> <li> <p>Go has a focus on performance and concurrency, with lightweight threads and efficient memory management. However:</p> <ul> <li>Go's focus on simplicity and minimalism can make it less expressive than some other languages, which can make some code less elegant and harder to read.</li> <li>Go's type system can be less flexible than other languages, which can be limiting for some projects.</li> <li>Go's tooling and package management system can be less user-friendly than other languages, especially for developers who are used to more sophisticated tools and frameworks.</li> </ul> </li> </ul>"},{"location":"the-case-for-go-backends/#conclusion","title":"Conclusion","text":"<p>Picking the right programming language is like choosing the right tool for the job when it comes to building top-notch, scalable, and budget-friendly backends. In this discussion, we've given TypeScript, Rust, and Go a look, discussing their performance and how much fun they bring to the developer's table in the world of backends and serverless apps.  Each of these languages has their own perks. Go, with its speedy, low-memory usage, might be the hero your backend project needs. But don't count out Rust and TypeScript \u2013 they're can handle the job just fine.</p> <p>In the end, it's all about finding that perfect fit for your specific project. Whether you're rolling with Go, Rust, or TypeScript, you're in good hands. \ud83d\ude80</p> <p>Why didn't I discuss Python, C#, Java, Ruby, or Elixir here?  Find out in my functional-versus-oop discussion.</p>"},{"location":"the-case-for-helm/","title":"The Case for Helm Charts Over DevSpace/Loft","text":"<p>DevSpace and Loft are both tools designed to simplify the development and deployment of applications on Kubernetes. Both tools provide features such as local development environments, simplified deployment workflows, and streamlined debugging and logging. However, they have some differences in their approach and feature sets. DevSpace is more focused on providing an end-to-end development workflow, while Loft emphasizes multi-tenancy and collaboration features for teams.</p> <p>Many of the features that DevSpace/Loft provides can be achieved by a more cost-effective approach: just by packaging and deploying Helm charts as normal.</p>"},{"location":"the-case-for-helm/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Helm</li> <li>Kind</li> <li>Open Lens</li> </ul>"},{"location":"the-case-for-helm/#helm","title":"Helm","text":"<p>Helm is a package manager for Kubernetes that allows you to package and deploy Kubernetes applications. Helm charts are a collection of files that describe a related set of Kubernetes resources. Helm charts can be used to deploy applications, but they can also be used to deploy other types of Kubernetes resources which are out of scope for this article.</p> <p>To install Helm, follow the instructons for your OS on the Helm website. Once Helm is installed, you can verify that it is working by running the following command:</p> <pre><code>helm version\n</code></pre> <p>Helm charts are stored in a Helm chart repository, which is a collection of Helm charts. Helm chart repositories can be hosted on a public or private server, or they can be hosted on a cloud storage service, such as Amazon S3 or Google Cloud Storage. Helm chart repositories can be accessed using the Helm CLI, which can be installed using the following command:</p> <pre><code>curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash\n</code></pre> <p>Once the Helm CLI is installed, you can add a Helm chart repository using the following command, for example:</p> <pre><code>helm repo add stable https://kubernetes-charts.storage.googleapis.com/\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo add janpreet https://janpreet.github.io/helm-charts/\nhelm repo update\n</code></pre> <p>Once a Helm chart repository has been added, you can search for Helm charts using the following command:</p> <pre><code>helm search repo stable\n\n# Once you've found a Helm chart that you want to install, you can install it using the following command:\nhelm install stable/mysql  \n</code></pre> <p>We installed the MySQL Helm chart just for demonstration purposes.  We will not be using it in this tutorial.</p>"},{"location":"the-case-for-helm/#kind","title":"Kind","text":"<p>Kind is a tool for running local Kubernetes clusters using Docker container \"nodes\". Kind was primarily designed for testing Kubernetes itself, but it can also be used for local development or CI.  Alternatives to Kind include Minikube, K3s, and MicroK8s.  However, Kind is the most popular option for local development due to its speed, ease of use, and it's ability to offer a local container registry (not in scope of this article).</p> <p>To install Kind, follow the instructions for your OS on the Kind website. Once Kind is installed, you can verify that it is working by running the following command:</p> <pre><code>kind version\n</code></pre> <p>Using namespaces is useful when you need to logically separate workloads within a single cluster, but still want to share resources such as nodes and network infrastructure. Namespace-level resource quotas can also be applied to prevent any one workload from using up too much of the shared resources.</p> <p>While Kubernetes namespaces are used to isolate workloads and resources within a single cluster, creating separate clusters with tools like Kind creates isolated environments with their own Kubernetes API server, etcd storage, and worker nodes. Each cluster can have its own configuration, resource allocation, and security settings. Separate clusters are also useful when you need to create isolated environments for different purposes, such as development, testing, or production. Each cluster can have its own resources and configurations, making it easier to manage and isolate workloads.</p> <p>With that said, let's create a namespace and cluster with the <code>kubectl</code> command.  <code>kubectl</code> is typically installed together with kind and minikube since it is a necessary command-line tool for interacting with Kubernetes clusters.  To create a these resources, run the following commands:</p> <pre><code># To isolate our tutorial application from other workloads, we'll create a separate Kind cluster\nkind create cluster --name tutorial-cluster\nkubectl config use-context kind-tutorial-cluster # switch to the kind cluster\n\n# create a namespace for all resources for our tutorial application\n# i.e., frontend, backend, database, etc.\nkubectl create namespace tutorial-app\n</code></pre>"},{"location":"the-case-for-helm/#open-lens","title":"Open Lens","text":"<p>OpenLens is an open-source project that provides a single dashboard to manage and monitor multiple Kubernetes clusters. It offers a graphical user interface that allows users to easily visualize and manage their resources. OpenLens supports a variety of Kubernetes distributions and is available for free.</p> <p>For more information about OpenLens, read this blog post and installation instructions from the OpenLens website.</p>"},{"location":"the-case-for-helm/#our-tutorial-application","title":"Our Tutorial Application","text":"<p>For this tutorial, we'll be using a simple Node.js application and \"Dockerize\" it. We can create this application by running the following commands:</p> <pre><code>mkdir tutorial-app &amp;&amp; cd $_\n\necho \"const http = require('http');\n\nconst hostname = '0.0.0.0';\nconst port = process.env.PORT || 3000;\n\nconst server = http.createServer((req, res) =&gt; {\n  res.statusCode = 200;\n  res.setHeader('Content-Type', 'text/plain');\n  res.end('Hello, world!\\n');\n});\n\nserver.listen(port, hostname, () =&gt; {\n  console.log(\\`Server running at http://\\${hostname}:\\${port}/\\`);\n});\" &gt;| app.js\n\necho 'FROM node:18-alpine\n\nWORKDIR /app\n\nCOPY . .\n\nEXPOSE 3000\n\nCMD [ \"node\", \"app.js\" ]' &gt;| Dockerfile\n</code></pre> <p>Subsequently, let's build and push our image to the Docker container registry. We can do this by running the following commands:</p> <pre><code>docker login # Login to Docker Hub\ndocker build . -t &lt;our Docker user name&gt;/tutorial-app\ndocker push &lt;our Docker user name&gt;/tutorial-app\n</code></pre> <p>Next, let's create a Helm chart for our application. We can do this by executing:</p> <pre><code>helm create tutorial-app\n\n# Replace the values.yml file with the following:\nexport our_DOCKER_USER_NAME=&lt;our Docker user name&gt;\nsed -i \"s/repository: nginx/repository: ${our_DOCKER_USER_NAME}\\/tutorial-app/g\" tutorial-app/values.yaml\nsed -i 's/port: 80/port: 3000/g' tutorial-app/values.yaml\n\n# Replace the deployment.yml file with the following:\nexport our_DOCKER_USER_NAME=&lt;our Docker user name&gt;\necho \"apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tutorial-app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tutorial-app\n  template:\n    metadata:\n      labels:\n        app: tutorial-app\n    spec:\n      containers:\n      - name: tutorial-app\n        image: ${our_DOCKER_USER_NAME}/tutorial-app\n        ports:\n        - containerPort: 3000\" &gt;| tutorial-app/templates/deployment.yaml\n</code></pre> <p>Lets deploy our Helm chart to the cluster:</p> <pre><code>helm install tutorial-app ./tutorial-app  --namespace tutorial-app # install the chart into the tutorial-app namespace\n</code></pre> <p>Deployment to the Kubernetes cluster is done. We can verify this by running the following command:</p> <pre><code>kubectl get pods --namespace tutorial-app\n</code></pre> <p>For security reasons, the port of our application isn't forwarded.  To forward the port of our application, run the following:</p> <pre><code>kubectl port-forward deployment/tutorial-app 3000:3000 --namespace tutorial-app &amp;\ncurl localhost:3000  #  you should see \"Hello, world!\"\n</code></pre> <p>To view this all in Open Lens, click the blue button representing cluster catalogs in the upper left corner of the Open Lens UI.  Then hen select the <code>kind-tutorial-cluster</code> to connect to it.  After connected, select <code>Deployments</code> in the new left sidebar. You should see the <code>tutorial-app</code> deployment. This will open a new dialog on the right-hand side of the UI. Within this new dialog, scroll to the \"3000/TCP\" clickable link. You should see the <code>Hello, world!</code> message open within your defaut web browser.</p>"},{"location":"the-case-for-helm/#troubleshooting","title":"Troubleshooting","text":"<p>If you have issues with steps in this tutorial, scorth earth and subsequently start over. We can do this by running the following commands:</p> <pre><code>kubectl delete namespace tutorial-app\nkind delete cluster --name tutorial-cluster\n\nkubectl create namespace tutorial-app\nkind create cluster --name tutorial-cluster\n\ndocker build -t &lt;our Docker user name&gt;/tutorial-app .\ndocker push &lt;our Docker user name&gt;/tutorial-app\n\nhelm install tutorial-app ./tutorial-app  --namespace tutorial-app\n</code></pre> <p>For remote clusters, I suggest using GitHub Actions to automate the deployment process.  We can use a GitHub Action to build and push a Docker image to the GitHub Container Registry (GHCR).  When we push to the GHCR, we can set it to automatically generate a webhook event that we can use to trigger actions in other systems.  For example, we can configure our deployment controller to periodically check the container registry for new versions of the image, or set up a webhook to trigger a deployment update whenever a new image is pushed to the registry.</p> <p>Another option is to use a continuous deployment tool like Flux or Argo CD, which can monitor our container registry and automatically update our Kubernetes cluster when new images are available. These tools can also help us manage the deployment process, automate rollbacks, and provide other useful features for managing our clusters.</p> <p>For more information on how this can be set up with Amazon's Elastic Kubernetes Service (EKS), see this article and this repository.</p> <p>Installing a Helm chart can be a one-time task with subsequent upgrades. My suggestion would be to create a separate repository for all Helm charts within our organization. It may not be as flashy as Loft, but it's a workable idea. We can have a repository for all our Helm charts, and we can use Flux or Argo CD to deploy them to our clusters. We can also use Flux or Argo CD to deploy our application manifests to our clusters. Since it's in a repository, we can have a subset of our team responsible for maintaining the Helm charts and application manifests. This way, we can have a separation of concerns between the developers who are less experienced in DevOps and others who are more experienced. These more \"DevOps\"-y engineers can be responsible for code reviews when pull requests are opened for this repository.</p>"},{"location":"the-case-for-helm/#automatic-application-deployment","title":"Automatic application deployment","text":"<p>DevSpace/Loft provides a feature that automatically deploys our application to a Kubernetes cluster whenever you make changes to our code. You can achieve this same functionality using Helm by using a CI/CD pipeline that automatically builds and deploys our Helm chart whenever changes are pushed to our source code repository.</p> <p>To achieve similar functionality to DevSpace's auto-deploy feature using Helm, you can set up a CI/CD pipeline that builds and deploys our Helm chart whenever changes are pushed to our source code repository. As mentioned earlier, we can use a continuous integration tool like GitHub Actions to build and push our Docker image to a container registry, and then use a continuous deployment tool like Flux or ArgoCD to automatically deploy our Helm chart to our Kubernetes cluster.</p>"},{"location":"the-case-for-helm/#local-development-environment","title":"Local development environment","text":"<p>DevSpace/Loft allows you to run a local development environment that mirrors the configuration of our production environment.</p> <p>Helm alone does not provide a way to create a local development environment that mirrors the production environment. However, we can achieve this same functionality wehen we deploy our application to a local Kubernetes cluster, such as Kind or Minikube, which can serve as a local development environment.  Using only Helm and a local Kubernetes cluster can be a more cost-effective approach than using additional tools like DevSpace or Loft.  Helm provides a simple and effective way to package, deploy, and manage applications on a Kubernetes cluster, while a local Kubernetes cluster like Minikube can provide a low-cost way to test and develop our application locally before deploying to a production environment.</p>"},{"location":"the-case-for-helm/#debugging-and-logging","title":"Debugging and logging","text":"<p>DevSpace/Loft provides features that allow you to easily debug and view logs from our application running in a Kubernetes cluster.</p> <p>Helm does not provide specific features for debugging or logging.  However, we can achieve this same functionality for our Helm-deployed applications by using Kubernetes logging and debugging tools, such as <code>kubectl logs</code>, Open Lens, or Kubernetes Dashboard.</p> <p>Using OpenLens, we can view the logs for our application by selecting the <code>kind-tutorial-cluster</code> cluster, and then selecting <code>Pods</code> in the sidebar.  We can then select the <code>tutorial-app*</code> pod, and then click on the <code>Logs</code> button in the dialog's upper toolbar.</p>"},{"location":"the-case-for-helm/#application-scaling","title":"Application scaling","text":"<p>DevSpace/Loft provides features that allow you to easily scale our application running in a Kubernetes cluster.</p> <p>We can achieve similar functionality as DevSpace/Loft for scaling our application in a Kubernetes cluster by using Helm. You can modify the <code>replicaCount</code> and <code>resources</code> fields in the values.yaml file to specify the desired number of replicas and resource requirements for our application.</p> <pre><code>replicaCount: 3\n\nresources:\n  limits:\n    cpu: \"1\"\n    memory: \"1Gi\"\n  requests:\n    cpu: \"500m\"\n    memory: \"256Mi\"\n</code></pre> <p>Then, when we install or upgrade our Helm chart using <code>helm install</code> or <code>helm upgrade</code> commands, the Kubernetes Deployment object that is created will have the desired number of replicas and resource requirements set according to the values specified in the <code>values.yaml</code> file.</p>"}]}